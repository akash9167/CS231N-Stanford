{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  # subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "  mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "  X_dev = X_train[mask]\n",
    "  y_dev = y_train[mask]\n",
    "  \n",
    "  # Preprocessing: reshape the image data into rows\n",
    "  X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "  X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "  X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "  \n",
    "  # Normalize the data: subtract the mean image\n",
    "  mean_image = np.mean(X_train, axis = 0)\n",
    "  X_train -= mean_image\n",
    "  X_val -= mean_image\n",
    "  X_test -= mean_image\n",
    "  X_dev -= mean_image\n",
    "  \n",
    "  # add bias dimension and transform into columns\n",
    "  X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "  X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "  X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "  X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "print 'dev data shape: ', X_dev.shape\n",
    "print 'dev labels shape: ', y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.384920\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print 'loss: %f' % loss\n",
    "print 'sanity check: %f' % (-np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 2.382698 analytic: 2.382698, relative error: 2.295807e-08\n",
      "numerical: -0.775293 analytic: -0.775293, relative error: 1.669248e-08\n",
      "numerical: 0.623417 analytic: 0.623417, relative error: 1.892317e-08\n",
      "numerical: 1.484528 analytic: 1.484528, relative error: 3.993702e-08\n",
      "numerical: 0.077493 analytic: 0.077493, relative error: 1.859958e-07\n",
      "numerical: -1.461922 analytic: -1.461922, relative error: 1.422905e-08\n",
      "numerical: 1.947172 analytic: 1.947172, relative error: 1.496993e-08\n",
      "numerical: -2.570632 analytic: -2.570632, relative error: 8.592508e-09\n",
      "numerical: -1.540230 analytic: -1.540230, relative error: 8.131805e-09\n",
      "numerical: 3.254156 analytic: 3.254156, relative error: 1.014728e-08\n",
      "numerical: 0.638400 analytic: 0.638400, relative error: 5.927548e-08\n",
      "numerical: -0.207499 analytic: -0.207499, relative error: 6.788631e-08\n",
      "numerical: -1.076235 analytic: -1.076236, relative error: 4.747543e-08\n",
      "numerical: -1.385460 analytic: -1.385460, relative error: 2.030956e-09\n",
      "numerical: 1.530300 analytic: 1.530300, relative error: 6.300128e-11\n",
      "numerical: 1.195927 analytic: 1.195927, relative error: 3.434897e-08\n",
      "numerical: 0.111242 analytic: 0.111242, relative error: 3.578276e-07\n",
      "numerical: -1.000690 analytic: -1.000690, relative error: 7.552265e-08\n",
      "numerical: 0.346042 analytic: 0.346042, relative error: 2.307135e-07\n",
      "numerical: 1.220535 analytic: 1.220535, relative error: 3.019778e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.359757e+00 computed in 0.217022s\n",
      "vectorized loss: 2.359757e+00 computed in 0.064909s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 778.910839\n",
      "iteration 100 / 1500: loss 285.503830\n",
      "iteration 200 / 1500: loss 105.748898\n",
      "iteration 300 / 1500: loss 40.080521\n",
      "iteration 400 / 1500: loss 16.019053\n",
      "iteration 500 / 1500: loss 7.201355\n",
      "iteration 600 / 1500: loss 4.009573\n",
      "iteration 700 / 1500: loss 2.768175\n",
      "iteration 800 / 1500: loss 2.293519\n",
      "iteration 900 / 1500: loss 2.223678\n",
      "iteration 1000 / 1500: loss 2.094399\n",
      "iteration 1100 / 1500: loss 2.064744\n",
      "iteration 1200 / 1500: loss 2.033310\n",
      "iteration 1300 / 1500: loss 2.093717\n",
      "iteration 1400 / 1500: loss 2.067838\n",
      "That took 9.133912s\n"
     ]
    }
   ],
   "source": [
    "# In the file linear_classifier.py, implement SGD in the function\n",
    "# LinearClassifier.train() and then run it with the code below.\n",
    "from cs231n.classifiers import Softmax\n",
    "softmax = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = softmax.train(X_train, y_train, learning_rate=1e-7, reg=5e4,\n",
    "                      num_iters=1500, verbose=True)\n",
    "toc = time.time()\n",
    "print 'That took %fs' % (toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm8AAAHuCAYAAADJMutoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJNWJ4kyRJqpH/B4Z5FiVlSPSaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f37dc288310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A useful debugging strategy is to plot the loss as a function of\n",
    "# iteration number:\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.000000e-10 reg 1.000000e-03 train accuracy: 0.101571 val accuracy: 0.094000\n",
      "lr 1.000000e-10 reg 1.000000e-02 train accuracy: 0.073143 val accuracy: 0.079000\n",
      "lr 1.000000e-10 reg 1.000000e-01 train accuracy: 0.116367 val accuracy: 0.132000\n",
      "lr 1.000000e-10 reg 1.000000e+00 train accuracy: 0.094673 val accuracy: 0.111000\n",
      "lr 1.000000e-10 reg 1.000000e+01 train accuracy: 0.074592 val accuracy: 0.085000\n",
      "lr 1.000000e-10 reg 1.000000e+02 train accuracy: 0.091490 val accuracy: 0.084000\n",
      "lr 1.000000e-10 reg 1.000000e+03 train accuracy: 0.085429 val accuracy: 0.099000\n",
      "lr 1.000000e-10 reg 1.000000e+04 train accuracy: 0.121898 val accuracy: 0.130000\n",
      "lr 1.000000e-10 reg 1.000000e+05 train accuracy: 0.082041 val accuracy: 0.074000\n",
      "lr 1.000000e-10 reg 1.000000e+06 train accuracy: 0.120959 val accuracy: 0.125000\n",
      "lr 1.668101e-08 reg 1.000000e-03 train accuracy: 0.097388 val accuracy: 0.101000\n",
      "lr 1.668101e-08 reg 1.000000e-02 train accuracy: 0.097469 val accuracy: 0.097000\n",
      "lr 1.668101e-08 reg 1.000000e-01 train accuracy: 0.125939 val accuracy: 0.142000\n",
      "lr 1.668101e-08 reg 1.000000e+00 train accuracy: 0.093204 val accuracy: 0.094000\n",
      "lr 1.668101e-08 reg 1.000000e+01 train accuracy: 0.084592 val accuracy: 0.089000\n",
      "lr 1.668101e-08 reg 1.000000e+02 train accuracy: 0.102592 val accuracy: 0.120000\n",
      "lr 1.668101e-08 reg 1.000000e+03 train accuracy: 0.110286 val accuracy: 0.113000\n",
      "lr 1.668101e-08 reg 1.000000e+04 train accuracy: 0.118673 val accuracy: 0.124000\n",
      "lr 1.668101e-08 reg 1.000000e+05 train accuracy: 0.107245 val accuracy: 0.108000\n",
      "lr 1.668101e-08 reg 1.000000e+06 train accuracy: 0.095714 val accuracy: 0.086000\n",
      "lr 2.782559e-06 reg 1.000000e-03 train accuracy: 0.095653 val accuracy: 0.085000\n",
      "lr 2.782559e-06 reg 1.000000e-02 train accuracy: 0.105122 val accuracy: 0.103000\n",
      "lr 2.782559e-06 reg 1.000000e-01 train accuracy: 0.100510 val accuracy: 0.105000\n",
      "lr 2.782559e-06 reg 1.000000e+00 train accuracy: 0.092367 val accuracy: 0.095000\n",
      "lr 2.782559e-06 reg 1.000000e+01 train accuracy: 0.114510 val accuracy: 0.119000\n",
      "lr 2.782559e-06 reg 1.000000e+02 train accuracy: 0.093878 val accuracy: 0.093000\n",
      "lr 2.782559e-06 reg 1.000000e+03 train accuracy: 0.095857 val accuracy: 0.096000\n",
      "lr 2.782559e-06 reg 1.000000e+04 train accuracy: 0.073551 val accuracy: 0.086000\n",
      "lr 2.782559e-06 reg 1.000000e+05 train accuracy: 0.102102 val accuracy: 0.092000\n",
      "lr 2.782559e-06 reg 1.000000e+06 train accuracy: 0.106347 val accuracy: 0.110000\n",
      "lr 4.641589e-04 reg 1.000000e-03 train accuracy: 0.099490 val accuracy: 0.084000\n",
      "lr 4.641589e-04 reg 1.000000e-02 train accuracy: 0.075347 val accuracy: 0.073000\n",
      "lr 4.641589e-04 reg 1.000000e-01 train accuracy: 0.112490 val accuracy: 0.092000\n",
      "lr 4.641589e-04 reg 1.000000e+00 train accuracy: 0.083796 val accuracy: 0.092000\n",
      "lr 4.641589e-04 reg 1.000000e+01 train accuracy: 0.123265 val accuracy: 0.120000\n",
      "lr 4.641589e-04 reg 1.000000e+02 train accuracy: 0.141429 val accuracy: 0.126000\n",
      "lr 4.641589e-04 reg 1.000000e+03 train accuracy: 0.086980 val accuracy: 0.100000\n",
      "lr 4.641589e-04 reg 1.000000e+04 train accuracy: 0.096857 val accuracy: 0.107000\n",
      "lr 4.641589e-04 reg 1.000000e+05 train accuracy: 0.089408 val accuracy: 0.088000\n",
      "lr 4.641589e-04 reg 1.000000e+06 train accuracy: 0.097673 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e-03 train accuracy: 0.072510 val accuracy: 0.060000\n",
      "lr 7.742637e-02 reg 1.000000e-02 train accuracy: 0.067469 val accuracy: 0.065000\n",
      "lr 7.742637e-02 reg 1.000000e-01 train accuracy: 0.117265 val accuracy: 0.113000\n",
      "lr 7.742637e-02 reg 1.000000e+00 train accuracy: 0.091082 val accuracy: 0.083000\n",
      "lr 7.742637e-02 reg 1.000000e+01 train accuracy: 0.120306 val accuracy: 0.114000\n",
      "lr 7.742637e-02 reg 1.000000e+02 train accuracy: 0.118388 val accuracy: 0.102000\n",
      "lr 7.742637e-02 reg 1.000000e+03 train accuracy: 0.130776 val accuracy: 0.116000\n",
      "lr 7.742637e-02 reg 1.000000e+04 train accuracy: 0.075102 val accuracy: 0.052000\n",
      "lr 7.742637e-02 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e-03 train accuracy: 0.080592 val accuracy: 0.080000\n",
      "lr 1.291550e+01 reg 1.000000e-02 train accuracy: 0.133816 val accuracy: 0.134000\n",
      "lr 1.291550e+01 reg 1.000000e-01 train accuracy: 0.130327 val accuracy: 0.136000\n",
      "lr 1.291550e+01 reg 1.000000e+00 train accuracy: 0.106388 val accuracy: 0.104000\n",
      "lr 1.291550e+01 reg 1.000000e+01 train accuracy: 0.134245 val accuracy: 0.120000\n",
      "lr 1.291550e+01 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e-03 train accuracy: 0.138265 val accuracy: 0.142000\n",
      "lr 2.154435e+03 reg 1.000000e-02 train accuracy: 0.077490 val accuracy: 0.071000\n",
      "lr 2.154435e+03 reg 1.000000e-01 train accuracy: 0.087163 val accuracy: 0.091000\n",
      "lr 2.154435e+03 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e-03 train accuracy: 0.078490 val accuracy: 0.075000\n",
      "lr 3.593814e+05 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.142000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [5e4, 1e8]\n",
    "learning_rates = np.logspace(-10, 10, 10) # np.logspace(-10, 10, 8) #-10, -9, -8, -7, -6, -5, -4\n",
    "regularization_strengths = np.logspace(-3, 6, 10) # causes numeric issues: np.logspace(-5, 5, 8) #[-4, -3, -2, -1, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "iters = 100\n",
    "for lr in learning_rates:\n",
    "    for rs in regularization_strengths:\n",
    "        softmax = Softmax()\n",
    "        softmax.train(X_train, y_train, learning_rate=lr, reg=rs, num_iters=iters)\n",
    "        \n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        acc_train = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        acc_val = np.mean(y_val == y_val_pred)\n",
    "        \n",
    "        results[(lr, rs)] = (acc_train, acc_val)\n",
    "        \n",
    "        if best_val < acc_val:\n",
    "            best_val = acc_val\n",
    "            best_softmax = softmax\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
